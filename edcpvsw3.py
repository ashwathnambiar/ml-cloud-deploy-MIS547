# -*- coding: utf-8 -*-
"""EdCpvsW3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ps00hFwoLwkV5LYbgKsDTi3Zy1oR3hzw

# Steam Dataset Clustering Analysis - Per Game Analysis

## Overview
Performing comprehensive K-means clustering analysis on Steam review data to segment players into different behavioral groups. **Each of the 3 games is analyzed separately** using 4 different clustering approaches.

### Datasets (Analyzed Separately):
- **Game 1091500**: Cyberpunk 2077 (32,700 reviews)
- **Game 1174180**: Red Dead Redemption 2 (8,907 reviews)
- **Game 292030**: The Witcher 3 (66,269 reviews)

### Analysis Structure:
**3 Games × 4 Clustering Types = 12 Total Analyses**

For EACH game, we perform:
1. **Feature 1**: Engagement & Activity Level
2. **Feature 2**: Positive vs Negative Reviewers
3. **Feature 3**: Playtime Patterns
4. **Feature 4**: Composite Player Profile
"""

# Import necessary libraries
import pandas as pd  # For data manipulation
import numpy as np  # For numerical operations
import matplotlib.pyplot as plt  # For visualization
import seaborn as sns  # For statistical visualizations
from sklearn.cluster import KMeans  # For K-means clustering
from sklearn.preprocessing import StandardScaler  # For feature normalization
from matplotlib.patches import Ellipse
from sklearn.decomposition import PCA  # For dimensionality reduction
from sklearn.metrics import silhouette_score  # For evaluating clustering quality
import warnings  # To suppress warnings
import openpyxl # Import openpyxl for reading .xlsx files
warnings.filterwarnings('ignore')  # Ignore warnings for cleaner output


# Set visualization style
sns.set_style("whitegrid")  # Set seaborn style
plt.rcParams['figure.figsize'] = (12, 8)  # Set default figure size

from google.colab import drive
drive.mount('/content/drive')

# Load all three CSV files from the Dataset folder
df_cyberpunk = pd.read_excel('/content/drive/MyDrive/Cyberpunk 2077_steam_reviews_1091500.part000_partial.xlsx')  # Load Cyberpunk 2077 reviews
df_rdr2 = pd.read_excel('/content/drive/MyDrive/Red Dead Redemption2_steam_reviews_1174180.part000_with_sent.xlsx')  # Load Red Dead Redemption 2 reviews
df_witcher = pd.read_excel('/content/drive/MyDrive/Witcher 3_steam_reviews_292030.part000_with_sent.xlsx')  # Load The Witcher 3 reviews

print("=" * 80)
print("DATASET OVERVIEW")
print("=" * 80)
print(f"Cyberpunk 2077:     {len(df_cyberpunk)} reviews")
print(f"Red Dead Redemption: {len(df_rdr2)} reviews")
print(f"The Witcher 3:      {len(df_witcher)} reviews")
print(f"\nTotal records:      {len(df_cyberpunk) + len(df_rdr2) + len(df_witcher)} reviews")

# Store game names for reference
games = {
    'cyberpunk': df_cyberpunk,
    'rdr2': df_rdr2,
    'witcher': df_witcher
}

# Display info for each game
for game_name, df_game in games.items():
    print(f"\n{game_name.upper()} - First 5 rows:")
    print(df_game[['author_steamid', 'author_num_games_owned', 'author_num_reviews',
                   'author_playtime_forever', 'voted_up']].head())

"""# Data Cleaning"""

def clean_data(df_game, english_only=True, winsorize=True):
    df = df_game.copy()

    print(f"\nStep 1: Original rows: {len(df)}")
# Language Filter English
    if english_only and 'author_language' in df.columns:
        before = len(df)
        df = df[df['author_language'].str.lower().str.contains('en', na=False)]
        print(f"Step 1a: Language filter (English): kept {len(df)}/{before}")
# Normalize True/False
    bool_like = {
        'voted_up', 'steam_purchase', 'received_for_free',
        'primarily_steam_deck', 'written_during_early_access'
    }
    for col in bool_like:
        if col in df.columns:
            df[col] = df[col].replace({True: 1, False: 0, 'True': 1, 'False': 0}).fillna(0).astype(int)
# Coerce numeric columns
    numeric_cols = [
        'author_num_games_owned', 'author_num_reviews',
        'author_playtime_forever', 'author_playtime_last_two_weeks',
        'author_playtime_at_review', 'author_deck_playtime_at_review',
        'votes_up', 'votes_funny', 'comment_count',
        'weighted_vote_score'
    ]
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    for col in ['author_playtime_last_two_weeks', 'author_playtime_at_review', 'author_deck_playtime_at_review']:
        if col in df.columns:
            df[col] = df[col].fillna(0)
# Sentiment encoding
    sentiment_map = {'negative': -1, 'neutral': 0, 'positive': 1}
    df['bert_sentiment_num'] = df.get('en_bert_label_from_partial', pd.Series(index=df.index)).map(sentiment_map).fillna(0)
    df['roberta_sentiment_num'] = df.get('en_roberta_label_from_partial', pd.Series(index=df.index)).map(sentiment_map).fillna(0)
    df['avg_sentiment'] = pd.to_numeric(df['bert_sentiment_num'], errors='coerce').fillna(0).to_numpy()
    df['avg_sentiment'] = (df['avg_sentiment'] + pd.to_numeric(df['roberta_sentiment_num'], errors='coerce').fillna(0)) / 2

# Parse timestamps
    for col in ['timestamp_created','timestamp_updated','timestamp_dev_responded']:
        if col in df.columns:
            df[f'{col}_dt'] = pd.to_datetime(df[col], unit='s', errors='coerce')

# De-duplicate
    before = len(df)
    if 'recommendationid' in df.columns:
        df = df.sort_values(by=['timestamp_updated'], ascending=False if 'timestamp_updated' in df.columns else True)
        df = df.drop_duplicates(subset=['recommendationid'], keep='first')
    elif {'author_steamid', 'review'}.issubset(df.columns):
        df = df.drop_duplicates(subset=['author_steamid', 'review'], keep='last')
    print(f"Step 2: After de-dup: {len(df)} rows (removed {before - len(df)})")

# Fix negatives & outliers
    for col in ['author_playtime_forever', 'author_playtime_last_two_weeks',
                'author_playtime_at_review', 'author_deck_playtime_at_review',
                'author_num_reviews', 'author_num_games_owned',
                'votes_up', 'votes_funny', 'comment_count',
                'community_participation', 'weighted_vote_score']:
        if col in df.columns:
            df[col] = df[col].clip(lower=0)

    if winsorize:
        heavy = [c for c in ['author_playtime_forever','author_playtime_last_two_weeks',
                             'author_num_games_owned','author_num_reviews','weighted_vote_score'] if c in df.columns]
        for c in heavy:
            lo, hi = df[c].quantile(0.01), df[c].quantile(0.99)
            df[c] = df[c].clip(lower=lo, upper=hi)
# Insure IDs
    if 'author_steamid' in df.columns:
        df['author_steamid'] = df['author_steamid'].astype(str)

# Fill remaining NAs used later
    for c in ['author_num_games_owned','author_num_reviews','author_playtime_forever','author_playtime_last_two_weeks',
              'author_playtime_at_review','author_deck_playtime_at_review','weighted_vote_score','avg_sentiment']:
        if c in df.columns:
            df[c] = df[c].fillna(0)

    print(f"Step 3: Clean complete: {len(df)} rows")
    return df

# Clean each game dataset
df_cyberpunk_clean = clean_data(df_cyberpunk)  # Clean Cyberpunk data
df_rdr2_clean = clean_data(df_rdr2)  # Clean RDR2 data
df_witcher_clean = clean_data(df_witcher)  # Clean Witcher data

print("=" * 80)
print("CLEANED DATASETS")
print("=" * 80)
print(f"Cyberpunk 2077:     {len(df_cyberpunk_clean)} valid records")
print(f"Red Dead Redemption: {len(df_rdr2_clean)} valid records")
print(f"The Witcher 3:      {len(df_witcher_clean)} valid records")

"""# Engineering Features"""

def create_features(df_clean: pd.DataFrame) -> pd.DataFrame:
    df = df_clean.copy()

    # Ensure presence
    for c in ['votes_up','votes_funny','comment_count','author_playtime_forever',
              'author_playtime_last_two_weeks','author_playtime_at_review','author_num_games_owned',
              'received_for_free','weighted_vote_score']:
        if c not in df.columns: df[c] = 0
        df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)

    # Community participation
    df['community_participation'] = (df['votes_up'] + df['votes_funny'] + df['comment_count']).astype(float)

    # Recent activity ratio (recent / forever)
    df['recent_activity_ratio'] = np.divide(
        df['author_playtime_last_two_weeks'], df['author_playtime_forever'],
        out=np.zeros_like(df['author_playtime_last_two_weeks'], dtype=float),
        where=(df['author_playtime_forever'] > 0)
    )

    # Purchase ratio (1 if purchased, 0 if free)
    df['purchase_ratio'] = np.where(df['received_for_free'] == 1, 0.0, 1.0)

    # Helpfulness & recent share
    denom = df['votes_up'] + df['votes_funny'] + df['comment_count'] + 1.0
    df['helpfulness_ratio'] = df['votes_up'] / denom

    base_play = df['author_playtime_at_review'].where(df['author_playtime_at_review'] > 0,
                                                      df['author_playtime_forever']).replace(0, np.nan)
    df['recent_playtime_share'] = np.divide(
        df['author_playtime_last_two_weeks'], base_play,
        out=np.zeros_like(df['author_playtime_last_two_weeks'], dtype=float),
        where=base_play.notna()
    )

    # Sentiment agreement (To sharpen the clusters)
    if {'bert_sentiment_num','roberta_sentiment_num'}.issubset(df.columns):
        df['sentiment_agreement'] = (df['bert_sentiment_num'] == df['roberta_sentiment_num']).astype(int)
    else:
        df['sentiment_agreement'] = 0

    return df

# Create features for each game
df_cyberpunk_features = create_features(df_cyberpunk_clean.copy())  # Add features to Cyberpunk
df_rdr2_features = create_features(df_rdr2_clean.copy())  # Add features to RDR2
df_witcher_features = create_features(df_witcher_clean.copy())  # Add features to Witcher

# Store cleaned and feature-engineered datasets
games_clean = {
    'cyberpunk': df_cyberpunk_features,
    'rdr2': df_rdr2_features,
    'witcher': df_witcher_features
}

print("\nSample of created features from Cyberpunk:")
df_cyberpunk_features[['author_num_games_owned', 'author_num_reviews',
                       'community_participation', 'recent_activity_ratio',
                       ]].head()

"""## Clustering Function Definition

We'll create a reusable function to perform all 4 clustering analyses on any game dataset.

"""

#Reuse Function
def _fit_kmeans_with_guard(X, desired_k, label_for_print):
    """
    Scale, fit KMeans with k guards, compute silhouette if valid.
    Returns: dict(labels, k, silhouette, features, skipped, skip_reason)
    """
    if isinstance(X, pd.DataFrame):
        features = X.columns.tolist()
        X = X.fillna(0).values
    else:
        features = None
    n = X.shape[0]

    if n == 0:
        print(f"  ✗ Skipped ({label_for_print}: no samples)")
        return dict(labels=None, k=None, silhouette=None, features=features, skipped=True, skip_reason="No samples")

    k = 1 if n == 1 else min(desired_k, n)

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    if k == 1:
        labels = np.zeros(n, dtype=int)
        sil = None
        print(f"  ✓ Completed (k=1, silhouette=n/a; reason: n={n} < desired_k={desired_k})")
        return dict(labels=labels, k=1, silhouette=sil, features=features, skipped=False, skip_reason=None)

    km = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = km.fit_predict(X_scaled)
    sil = silhouette_score(X_scaled, labels) if len(np.unique(labels)) > 1 else None
    print(f"  ✓ Completed (k={k}, silhouette={sil if sil is not None else 'n/a'})")
    return dict(labels=labels, k=k, silhouette=sil, features=features, skipped=False, skip_reason=None)

def _encode_sentiment_cols(df):
    """Adds bert_sentiment_num, roberta_sentiment_num, avg_sentiment (safe)."""
    sentiment_map = {'negative': -1, 'neutral': 0, 'positive': 1}
    bert = df.get('en_bert_label_from_partial', None)
    rob  = df.get('en_roberta_label_from_partial', None)
    df = df.copy()
    df['bert_sentiment_num']    = bert.map(sentiment_map).fillna(0) if bert is not None else 0
    df['roberta_sentiment_num'] = rob.map(sentiment_map).fillna(0)  if rob  is not None else 0
    df['avg_sentiment'] = (pd.to_numeric(df['bert_sentiment_num'], errors='coerce').fillna(0) +
                           pd.to_numeric(df['roberta_sentiment_num'], errors='coerce').fillna(0)) / 2
    return df

"""# FEATURE 1: ENGAGEMENT & ACTIVITY LEVEL"""

def cluster_engagement(df_game):
    """Feature 1: Engagement & Activity Level"""
    print("\n[Feature 1] Engagement & Activity Level...")

    # Transform heavy-tailed counts
    df = df_game.copy()
    df['log_games_owned']  = np.log1p(df['author_num_games_owned'].fillna(0))
    df['log_num_reviews']  = np.log1p(df['author_num_reviews'].fillna(0))

    # Z-score playtime intensity
    play = df['author_playtime_forever'].fillna(0).astype(float)
    mu, sigma = play.mean(), play.std(ddof=0)
    df['playtime_z'] = (play - mu) / (sigma if sigma > 0 else 1.0)

    features = ['log_games_owned', 'log_num_reviews', 'playtime_z', 'community_participation']
    X = df[features].fillna(0)

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    desired_k = 4
    results = _fit_kmeans_with_guard(X_scaled, desired_k, "engagement")

    # Extract values from the results dictionary
    labels = results['labels']
    silhouette = results['silhouette']
    skipped = results['skipped']
    reason = results['skip_reason']

    results = {
        'labels': labels,
        'silhouette': silhouette,
        'features': features,
        'skipped': skipped,
        'skip_reason': reason,
        'k': results['k'] # Add the actual k used
    }

    return results

"""# Feature 2 — Reviewer Sentiment & Behavior"""

def cluster_reviewer_sentiment(df_game):
    """Feature 2: Reviewer Sentiment & Behavior"""
    print("\n[Feature 2] Reviewer Sentiment & Behavior...")

    df = df_game.copy()
    sentiment_map = {'negative': -1, 'neutral': 0, 'positive': 1}
    df['bert_sentiment_num'] = df.get('en_bert_label_from_partial', pd.Series(index=df.index)).map(sentiment_map).fillna(0)
    df['roberta_sentiment_num'] = df.get('en_roberta_label_from_partial', pd.Series(index=df.index)).map(sentiment_map).fillna(0)
    df['avg_sentiment'] = (df['bert_sentiment_num'] + df['roberta_sentiment_num']) / 2

    player_stats = df.groupby('author_steamid', as_index=False).agg({
        'avg_sentiment': 'mean',
        'author_num_reviews': 'first',
        'weighted_vote_score': 'mean'
    })
    player_stats.rename(columns={'author_num_reviews':'total_reviews','weighted_vote_score':'avg_vote_score'}, inplace=True)

    features = ['avg_sentiment','total_reviews','avg_vote_score']
    X = player_stats[features].fillna(0)

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    desired_k = 3
    results = _fit_kmeans_with_guard(X_scaled, desired_k, "reviewer")

    # Extract values from the results dictionary
    labels = results['labels']
    silhouette = results['silhouette']
    skipped = results['skipped']
    reason = results['skip_reason']


    results = {
        'labels': labels,
        'silhouette': silhouette,
        'features': features,
        'player_stats': player_stats,
        'skipped': skipped,
        'skip_reason': reason,
        'k': results['k'] # Add the actual k used
    }

    return results

"""# Feature 3 — Playtime Patterns"""

def cluster_playtime_patterns(df_game):
    """Feature 3: Playtime Patterns"""
    print("\n[Feature 3] Playtime Patterns...")

    df = df_game.copy()
    play = df['author_playtime_forever'].fillna(0).astype(float)
    mu, sigma = play.mean(), play.std(ddof=0)
    df['playtime_z'] = (play - mu) / (sigma if sigma > 0 else 1.0)
    df['playtime_pct'] = play.rank(method='average', pct=True)

    base_play = df['author_playtime_at_review'].where(
        df['author_playtime_at_review'].fillna(0) > 0,
        df['author_playtime_forever'].fillna(0)
    ).replace(0, np.nan)
    df['recent_playtime_share'] = np.divide(
        df['author_playtime_last_two_weeks'].fillna(0),
        base_play,
        out=np.zeros_like(df['author_playtime_last_two_weeks'].fillna(0), dtype=float),
        where=base_play.notna()
    )

    features = [
        'author_playtime_forever', 'author_playtime_last_two_weeks',
        'recent_activity_ratio','playtime_z','playtime_pct','recent_playtime_share'
    ]
    X = df[features].fillna(0)

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    desired_k = 4
    results = _fit_kmeans_with_guard(X_scaled, desired_k, "playtime")

    # Extract values from the results dictionary
    labels = results['labels']
    silhouette = results['silhouette']
    skipped = results['skipped']
    reason = results['skip_reason']

    results = {
        'labels': labels,
        'silhouette': silhouette,
        'features': features,
        'skipped': skipped,
        'skip_reason': reason,
        'k': results['k'] # Add the actual k used
    }

    return results

"""# Feature 4 — Composite Player Profile"""

def cluster_composite_profile(df_game):
    """Feature 4: Composite Player Profile"""
    print("\n[Feature 4] Composite Player Profile...")

    df = df_game.copy()
    denom = (df['votes_up'].fillna(0) + df['votes_funny'].fillna(0) + df['comment_count'].fillna(0) + 1.0)
    df['helpfulness_ratio'] = df['votes_up'].fillna(0) / denom
    df['log_games_owned'] = np.log1p(df['author_num_games_owned'].fillna(0))
    df['log_num_reviews'] = np.log1p(df['author_num_reviews'].fillna(0))

    player_composite = df.groupby('author_steamid', as_index=False).agg({
        'log_games_owned':'first',
        'log_num_reviews':'first',
        'author_playtime_forever':'first',
        'community_participation':'sum',
        'avg_sentiment':'mean',
        'helpfulness_ratio':'mean',
        'voted_up':'mean',
        'steam_purchase':'mean',
        'received_for_free':'mean',
        'recent_activity_ratio':'mean'
    })

    features = [
        'log_games_owned','log_num_reviews','author_playtime_forever',
        'community_participation','avg_sentiment','helpfulness_ratio',
        'voted_up','steam_purchase','received_for_free','recent_activity_ratio'
    ]
    X = player_composite[features].fillna(0)

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    desired_k = 5
    results = _fit_kmeans_with_guard(X_scaled, desired_k, "composite")

    # Extract values from the results dictionary
    labels = results['labels']
    silhouette = results['silhouette']
    skipped = results['skipped']
    reason = results['skip_reason']

    results = {
        'labels': labels,
        'silhouette': silhouette,
        'features': features,
        'player_composite': player_composite,
        'skipped': skipped,
        'skip_reason': reason,
        'k': results['k'] # Add the actual k used
    }

    return results

"""# Combine"""

def perform_clustering_analysis(df_game, game_name):
    print("\n" + "=" * 80)
    print(f"ANALYZING {game_name.upper()}")
    print("=" * 80)

    results = {
        'engagement': cluster_engagement(df_game),
        'reviewer': cluster_reviewer_sentiment(df_game),
        'playtime': cluster_playtime_patterns(df_game),
        'composite': cluster_composite_profile(df_game),
        'game_name': game_name,
        'original_df': df_game
    }
    return results

"""## Run Clustering Analysis on All 3 Games

Now we'll apply our clustering function to each game separately.

"""

# Execute clustering analysis for all three games
print("=" * 80)
print("STARTING CLUSTERING ANALYSIS FOR SELECTED GAMES")
print("=" * 80)

results_witcher = perform_clustering_analysis(games_clean['witcher'], 'The Witcher 3')
results_cyberpunk = perform_clustering_analysis(games_clean['cyberpunk'], 'Cyberpunk 2077')
results_rdr2 = perform_clustering_analysis(games_clean['rdr2'], 'Red Dead Redemption 2')

# Store results in a dictionary for later comparison/visualization
all_results = {
    'The Witcher 3': results_witcher,
    'Cyberpunk 2077': results_cyberpunk,
    'Red Dead Redemption 2': results_rdr2
}

print("\n" + "=" * 80)
print("SELECTED CLUSTERING ANALYSES COMPLETED")
print("=" * 80)

"""## Results Comparison Across Games

Compare the clustering results across all three games.

"""

# Helpers
def _fmt_sil(s):
    return "n/a" if (s is None or (isinstance(s, float) and np.isnan(s))) else f"{s:.3f}"

def _cluster_counts_legend(ax, labels):
    if labels is None or len(np.unique(labels)) <= 1:
        ax.legend([], title="Clusters", frameon=False, loc="best")
        return
    vals, cnts = np.unique(labels, return_counts=True)
    txt = "\n".join([f"C{int(v)}: {int(c)}" for v, c in zip(vals, cnts)])
    ax.text(0.98, 0.02, txt, transform=ax.transAxes, ha="right", va="bottom",
            fontsize=9, bbox=dict(boxstyle="round,pad=0.25", fc="white", ec="#888", alpha=0.8))

# def _ellipse_for_points(points, n_std=2.0):
#    """Return (width, height, angle_deg, center_x, center_y) for covariance ellipse."""
#    if points.shape[0] < 2:
#        return None
#    cov = np.cov(points.T)
#    vals, vecs = np.linalg.eigh(cov)
#    order = np.argsort(vals)[::-1]
#    vals, vecs = vals[order], vecs[:, order]
#    angle = np.degrees(np.arctan2(*vecs[:, 0][::-1]))
#    width, height = 2 * n_std * np.sqrt(np.maximum(vals, 1e-12))
#    center = points.mean(axis=0)
#    return width, height, angle, center[0], center[1]

def _draw_cluster_shapes(ax, XY, labels, alpha=0.10):
    if labels is None or len(np.unique(labels)) <= 1:
        return
    for k in np.unique(labels):
        pts = XY[labels == k]
        ell = _ellipse_for_points(pts)
        if ell is None:
            continue
        w, h, ang, cx, cy = ell
        e = Ellipse((cx, cy), width=w, height=h, angle=ang, fill=True, alpha=alpha, ec="none")
        ax.add_patch(e)
        # centroid
        ax.scatter([cx], [cy], marker="D", s=60, edgecolors="black", linewidths=0.6)

def _clip_percentiles(arr, lo=1, hi=99):
    lo_v, hi_v = np.percentile(arr, [lo, hi])
    return np.clip(arr, lo_v, hi_v)

def _pca_panel(ax, X_df, labels, title_prefix, cmap_name, silhouette, show_loadings=True):
    # guard no data
    if X_df is None or len(X_df) == 0:
        ax.text(0.5, 0.5, "No samples", ha="center", va="center", fontsize=12)
        ax.set_title(f"{title_prefix} (Silhouette: {_fmt_sil(silhouette)})", fontsize=12, fontweight='bold')
        ax.axis('off'); return

    # clip outliers column-wise (so dense mass is visible)
    Xc = X_df.copy()
    for c in Xc.columns:
        Xc[c] = _clip_percentiles(pd.to_numeric(Xc[c], errors='coerce').fillna(0).values, 1, 99)

    # scale -> pca
    X_scaled = StandardScaler().fit_transform(Xc.values)
    n_samples, n_features = X_scaled.shape
    n_comp = 2 if min(n_samples, n_features) >= 2 else 1
    pca = PCA(n_components=n_comp)
    X_p = pca.fit_transform(X_scaled)

    # scatter
    if n_comp == 1:
        ax.scatter(X_p.ravel(), np.zeros_like(X_p.ravel()),
                   c=labels if (labels is not None and len(np.unique(labels)) > 1) else "#777777",
                   cmap=cmap_name, s=36, alpha=0.75, edgecolors="black", linewidth=0.4)
        ax.set_xlabel(f"PC1 ({(getattr(pca, 'explained_variance_ratio_', [0])[0]):.1%})")
        ax.set_yticks([])
    else:
        ax.scatter(X_p[:, 0], X_p[:, 1],
                   c=labels if (labels is not None and len(np.unique(labels)) > 1) else "#777777",
                   cmap=cmap_name, s=36, alpha=0.75, edgecolors="black", linewidth=0.4)
        ax.set_xlabel(f"PC1 ({pca.explained_variance_ratio_[0]:.1%})")
        ax.set_ylabel(f"PC2 ({pca.explained_variance_ratio_[1]:.1%})")

        # cluster ellipses + centroids
        if labels is not None and len(np.unique(labels)) > 1:
            # _draw_cluster_shapes(ax, X_p, labels, alpha=0.12)
            for k in np.unique(labels):
              pts = X_p[labels == k]
              cx, cy = pts.mean(axis=0)
              ax.scatter([cx], [cy], marker="D", s=60, edgecolors="black", linewidths=0.6)

        # optional loadings (show top contributing features)
        if show_loadings:
            loadings = pca.components_.T  # shape [features, 2]
            # pick top |loading| features up to 6 arrows
            mag = np.linalg.norm(loadings, axis=1)
            idx = np.argsort(mag)[::-1][:min(6, len(mag))]
            scale = 2.0  # arrow length scale in the PC space
            for i in idx:
                v = loadings[i] * scale
                ax.arrow(0, 0, v[0], v[1], head_width=0.05, head_length=0.08, fc='k', ec='k', alpha=0.4, linewidth=0.8)
                ax.text(v[0]*1.1, v[1]*1.1, X_df.columns[i], fontsize=8, color="#555", alpha=0.7)

    # legend with counts
    _cluster_counts_legend(ax, labels)
    ax.grid(True, alpha=0.25)
    ax.set_title(f"{title_prefix} (Silhouette: {_fmt_sil(silhouette)})", fontsize=12, fontweight='bold')

def visualize_four_fix1(result_game, game_name="(Game)", show_loadings=True):
    """Pretty 2x2 PCA panels with centroids, ellipses, counts, clipping, and optional loadings."""
    fig, axes = plt.subplots(2, 2, figsize=(17, 14))
    axes = axes.ravel()

    # Feature 1 (review-level)
    f1 = result_game.get('engagement', {})
    feats1 = f1.get('features', [])
    # Recreate necessary features for visualization
    df1 = result_game['original_df'].copy()
    if 'author_num_games_owned' in df1.columns:
        df1['log_games_owned'] = np.log1p(df1['author_num_games_owned'].fillna(0))
    if 'author_num_reviews' in df1.columns:
        df1['log_num_reviews'] = np.log1p(df1['author_num_reviews'].fillna(0))
    if 'author_playtime_forever' in df1.columns:
        play = df1['author_playtime_forever'].fillna(0).astype(float)
        mu, sigma = play.mean(), play.std(ddof=0)
        df1['playtime_z'] = (play - mu) / (sigma if sigma > 0 else 1.0)
    if 'community_participation' not in df1.columns and all(col in df1.columns for col in ['votes_up', 'votes_funny', 'comment_count']):
         df1['community_participation'] = (df1['votes_up'].fillna(0) + df1['votes_funny'].fillna(0) + df1['comment_count'].fillna(0)).astype(float)

    X1 = df1[feats1].fillna(0) if feats1 and not df1.empty else pd.DataFrame()
    _pca_panel(axes[0], X1, f1.get('labels'), "Feature 1: Engagement", "viridis", f1.get('silhouette'), show_loadings)

    # Feature 2 (player-level)
    f2 = result_game.get('reviewer', {})
    feats2 = f2.get('features', [])
    ps = f2.get('player_stats', pd.DataFrame())
    X2 = ps[feats2].fillna(0) if (feats2 and isinstance(ps, pd.DataFrame) and not ps.empty) else pd.DataFrame()
    _pca_panel(axes[1], X2, f2.get('labels'), "Feature 2: Reviewer Type", "coolwarm", f2.get('silhouette'), show_loadings)

    # Feature 3 (review-level)
    f3 = result_game.get('playtime', {})
    feats3 = f3.get('features', [])
    # Recreate necessary features for visualization
    df3 = result_game['original_df'].copy()
    if 'author_playtime_forever' in df3.columns:
        play = df3['author_playtime_forever'].fillna(0).astype(float)
        mu, sigma = play.mean(), play.std(ddof=0)
        df3['playtime_z'] = (play - mu) / (sigma if sigma > 0 else 1.0)
        df3['playtime_pct'] = play.rank(method='average', pct=True)
    if all(col in df3.columns for col in ['author_playtime_at_review', 'author_playtime_forever', 'author_playtime_last_two_weeks']):
        base_play = df3['author_playtime_at_review'].where(
            df3['author_playtime_at_review'].fillna(0) > 0,
            df3['author_playtime_forever'].fillna(0)
        ).replace(0, np.nan)
        df3['recent_playtime_share'] = np.divide(
            df3['author_playtime_last_two_weeks'].fillna(0),
            base_play,
            out=np.zeros_like(df3['author_playtime_last_two_weeks'].fillna(0), dtype=float),
            where=base_play.notna()
        )
    if 'recent_activity_ratio' not in df3.columns and all(col in df3.columns for col in ['author_playtime_last_two_weeks', 'author_playtime_forever']):
        df3['recent_activity_ratio'] = np.divide(
            df3['author_playtime_last_two_weeks'], df3['author_playtime_forever'],
            out=np.zeros_like(df3['author_playtime_last_two_weeks'], dtype=float),
            where=(df3['author_playtime_forever'] > 0)
        )

    X3 = df3[feats3].fillna(0) if feats3 and not df3.empty else pd.DataFrame()
    _pca_panel(axes[2], X3, f3.get('labels'), "Feature 3: Playtime Patterns", "plasma", f3.get('silhouette'), show_loadings)

    # Feature 4 (player-level)
    f4 = result_game.get('composite', {})
    feats4 = f4.get('features', [])
    player_comp = f4.get('player_composite', pd.DataFrame())
    X4 = player_comp[feats4].fillna(0) if (feats4 and isinstance(player_comp, pd.DataFrame) and not player_comp.empty) else pd.DataFrame()
    _pca_panel(axes[3], X4, f4.get('labels'), "Feature 4: Composite Profile", "Set2", f4.get('silhouette'), show_loadings)

    plt.suptitle(f"{game_name} — All Clustering Analyses", fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()

visualize_four_fix1(results_witcher, "The Witcher 3", show_loadings=True)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

def _fmt_sil(s):
    return "n/a" if (s is None or (isinstance(s, float) and np.isnan(s))) else f"{s:.3f}"

def _cluster_counts_legend(ax, labels):
    if labels is None:
        return
    vals, cnts = np.unique(labels, return_counts=True)
    if len(vals) <= 1:
        return
    txt = "\n".join([f"C{int(v)}: {int(c)}" for v, c in zip(vals, cnts)])
    ax.text(0.98, 0.02, txt, transform=ax.transAxes, ha="right", va="bottom",
            fontsize=9, bbox=dict(boxstyle="round,pad=0.25", fc="white", ec="#888", alpha=0.8))

# =========================
#   3-Panel Explainer Plot
# =========================
# Select a game to visualize, e.g., The Witcher 3, Cyberpunk 2077, Red Dead Redemption 2
game_name_to_visualize = "The Witcher 3"
results_game = all_results.get(game_name_to_visualize)

if not results_game:
    print(f"Clustering results for {game_name_to_visualize} not found.")
else:
    f3 = results_game.get('playtime', {}) or {}
    feats3 = f3.get('features', []) or []

    if not feats3:
        print(f"No playtime features found for {game_name_to_visualize}.")
    else:
        # ----- Recreate engineered features from original_df (so plots match your pipeline) -----
        df3 = results_game['original_df'].copy()

        # Lifetime playtime features
        if 'author_playtime_forever' in df3.columns:
            play = pd.to_numeric(df3['author_playtime_forever'], errors='coerce').fillna(0.0)
            mu, sigma = play.mean(), (play.std(ddof=0) or 1.0)
            df3['playtime_z'] = (play - mu) / sigma
            df3['playtime_pct'] = play.rank(method='average', pct=True)

        # Recent playtime share (prefers at_review baseline if available)
        if all(c in df3.columns for c in ['author_playtime_at_review','author_playtime_forever','author_playtime_last_two_weeks']):
            at_review = pd.to_numeric(df3['author_playtime_at_review'], errors='coerce')
            forever  = pd.to_numeric(df3['author_playtime_forever'], errors='coerce').fillna(0.0)
            recent   = pd.to_numeric(df3['author_playtime_last_two_weeks'], errors='coerce').fillna(0.0)
            base = at_review.where(at_review.fillna(0) > 0, forever).replace(0, np.nan)
            df3['recent_playtime_share'] = np.divide(
                recent, base,
                out=np.zeros_like(recent, dtype=float),
                where=base.notna()
            )

        # Recent / lifetime ratio
        if all(c in df3.columns for c in ['author_playtime_last_two_weeks','author_playtime_forever']):
            recent = pd.to_numeric(df3['author_playtime_last_two_weeks'], errors='coerce').fillna(0.0)
            forever = pd.to_numeric(df3['author_playtime_forever'], errors='coerce').fillna(0.0)
            df3['recent_activity_ratio'] = np.divide(
                recent, forever,
                out=np.zeros_like(recent, dtype=float),
                where=(forever > 0)
            )

        # ----- Assemble PCA matrix (exact features your model used) -----
        X3 = df3[feats3].apply(pd.to_numeric, errors='coerce').fillna(0.0)
        if X3.empty or len(X3) < 2 or X3.shape[1] == 0:
            print("Not enough data to plot.")
        else:
            # ----- Fit ONE PCA used by both the PCA scatter and the loadings chart -----
            scaler = StandardScaler()
            X3_scaled = scaler.fit_transform(X3.values)
            n_samples, n_features = X3_scaled.shape
            n_comp = 2 if min(n_samples, n_features) >= 2 else 1
            pca = PCA(n_components=n_comp).fit(X3_scaled)
            X3_pca = pca.transform(X3_scaled)
            labels = f3.get('labels')

            # Loadings (feature contributions) dataframe
            loadings = pca.components_.T  # [features, n_comp]
            load_cols = [f"PC{i+1}" for i in range(n_comp)]
            loadings_df = pd.DataFrame(loadings, index=X3.columns, columns=load_cols)

            # ----- Plot: 3 panels -----
            fig, axs = plt.subplots(1, 3, figsize=(18, 5))

            # (1) Left: PCA scatter (uses the SAME pca as the middle panel)
            if n_comp == 1:
                x, y = X3_pca.ravel(), np.zeros_like(X3_pca.ravel())
            else:
                x, y = X3_pca[:, 0], X3_pca[:, 1]

            axs[0].scatter(
                x, y,
                c=labels if (labels is not None and len(np.unique(labels)) > 1) else "#777777",
                cmap="Spectral", s=22, alpha=0.75, edgecolors="none"
            )
            # Optional: centroids (diamonds)
            if labels is not None and len(np.unique(labels)) > 1:
                for k in np.unique(labels):
                    mask = (labels == k)
                    cx, cy = x[mask].mean(), y[mask].mean()
                    axs[0].scatter([cx], [cy], marker="D", s=58, edgecolors="black", linewidths=0.6, c="#ffffff")

            # Axis labels with explained variance
            if n_comp >= 1:
                axs[0].set_xlabel(f"PC1 ({pca.explained_variance_ratio_[0]:.1%})")
            if n_comp >= 2:
                axs[0].set_ylabel(f"PC2 ({pca.explained_variance_ratio_[1]:.1%})")
            else:
                axs[0].set_yticks([])

            axs[0].set_title(f"Playtime Patterns ({game_name_to_visualize}) (Silhouette: {_fmt_sil(f3.get('silhouette'))})")
            axs[0].grid(True, alpha=0.25)
            _cluster_counts_legend(axs[0], labels)

            # (2) Middle: PCA loadings (which features define PC1 / PC2?)
            ax_mid = axs[1]

            top_n = min(10, n_features)

            # Rank features by absolute loading on PC1
            abs_pc1 = loadings_df["PC1"].abs()
            top_feats = abs_pc1.sort_values(ascending=False).head(top_n).index

            # Slice for those features
            load_sub = loadings_df.loc[top_feats]

            load_sub = load_sub.iloc[::-1]

            y_pos = np.arange(len(load_sub))

            if n_comp == 1:
                # Only PC1 available – simple bar chart
                ax_mid.barh(y_pos, load_sub["PC1"])
                ax_mid.set_xlabel("Loading on PC1")
            else:
                # PC1 and PC2 – side-by-side bars
                width = 0.4
                ax_mid.barh(y_pos - width/2, load_sub["PC1"], height=width, label="PC1")
                ax_mid.barh(y_pos + width/2, load_sub["PC2"], height=width, label="PC2")
                ax_mid.set_xlabel("PCA loadings")

            ax_mid.set_yticks(y_pos)
            ax_mid.set_yticklabels(load_sub.index)
            ax_mid.set_title("Key features driving PC1/PC2")
            ax_mid.axvline(0, color="#444444", linewidth=0.8)
            ax_mid.grid(True, axis="x", alpha=0.2)

            if n_comp >= 2:
                ax_mid.legend(loc="lower right", fontsize=9)


            # (3) Right: Real-world axes (lifetime vs recent playtime)
            if all(c in df3.columns for c in ['author_playtime_forever','author_playtime_last_two_weeks']):
                x_hours = pd.to_numeric(df3['author_playtime_forever'], errors='coerce').fillna(0.0) / 60.0
                y_hours = pd.to_numeric(df3['author_playtime_last_two_weeks'], errors='coerce').fillna(0.0) / 60.0
                axs[2].scatter(
                    x_hours, y_hours,
                    c=labels if (labels is not None and len(np.unique(labels)) > 1) else "#777777",
                    cmap="Spectral", s=22, alpha=0.75, edgecolors="none"
                )
                axs[2].set_xscale("log"); axs[2].set_yscale("log")
                axs[2].set_xlabel("Lifetime playtime (hours, log)")
                axs[2].set_ylabel("Recent playtime — last 2 weeks (hours, log)")
                axs[2].set_title("Clusters on real axes")
                axs[2].grid(True, alpha=0.25)
            else:
                axs[2].text(0.5, 0.5, "Playtime columns not found", ha="center", va="center", fontsize=12)
                axs[2].set_title("Clusters on real axes")
                axs[2].axis('off')


            plt.tight_layout()
            plt.show()

"""Each dot = one player or review, depending on the feature group.
Color = cluster label found by K-Means.
Ellipses = rough 95 % region of that cluster.
Diamond = cluster centroid (mean).
Axes (PC 1 & PC 2) = the two directions of greatest variance in the feature space.

Silhouette ≈ 0 → clusters mixed; ≈ 0.5 → moderate structure; > 0.6 → clear, compact clusters.
"""

# Create comparison summary
# all_results = {
#     'The Witcher 3': results_witcher
# }

# Display summary table
print("\n" + "=" * 100)
print("CLUSTERING RESULTS SUMMARY - COMPARISON ACROSS GAMES")
print("=" * 100)

for game_name, results in all_results.items():
    print(f"\n{game_name}:")
    print(f"  Feature 1 (Engagement):     k={results['engagement']['k']}, silhouette={results['engagement']['silhouette']:.3f}")
    print(f"  Feature 2 (Reviewer):       k={results['reviewer']['k']}, silhouette={results['reviewer']['silhouette']:.3f}")
    print(f"  Feature 3 (Playtime):       k={results['playtime']['k']}, silhouette={results['playtime']['silhouette']:.3f}")
    print(f"  Feature 4 (Composite):     k={results['composite']['k']}, silhouette={results['composite']['silhouette']:.3f}")

# Calculate averages
print("\n" + "=" * 100)
print("AVERAGE SILHOUETTE SCORES:")
print("=" * 100)
avg_silhouettes = {
    'Engagement': np.mean([r['engagement']['silhouette'] for r in all_results.values()]),
    'Reviewer': np.mean([r['reviewer']['silhouette'] for r in all_results.values()]),
    'Playtime': np.mean([r['playtime']['silhouette'] for r in all_results.values()]),
    'Composite': np.mean([r['composite']['silhouette'] for r in all_results.values()])
}

for feature, score in avg_silhouettes.items():
    print(f"{feature:15s}: {score:.3f}")

"""# Pivote
"Accused of "burying" reviews, or manipulating users into only seeing reviews that affirm their pre-existing beliefs."

"How can we categorize and identify reviewers behavior, evaluate patterns across video games that share similar tags, and use these information to predict personal sentiment toward future release?"

"Cyberpunk 2077" and "The Witcher 3" are both developed and published by "CD PROJEKT RED" sharing multiple tags such as "Open World", "RPG", "Singleplayer", "Story Rich", "Exploration", "Action" and more. We can say they have similar playstyle with one major difference "Background Settings", a Future Sci-fi comparing to a Dark Fantasy Magic world.

Categorize reviewers based on behavioral similarity using engagement metrics, playtime patterns, sentiment tendencies, and community interaction.

Evaluate whether the same personas appear across different CDPR games.

Predict how each persona is likely to review a new game:
forecast sentiment, identify high-risk reviewer groups, and tailor communication and marketing strategies.

## The Witcher 3 vs Cyberpunk 2077
"""

import numpy as np
import pandas as pd

def build_modeling_table_for_game(game_key: str, all_results: dict) -> pd.DataFrame:
    """
    Build a per-review modeling table for one game from all_results.
    - game_key must be one of: 'Cyberpunk 2077', 'The Witcher 3', 'Red Dead Redemption 2'
    - Each row = one review.
    - Includes:
        * cluster labels (engagement, playtime, reviewer, composite)
        * selected numeric features you engineered
        * label y (voted_up or fallback to sentiment)
    """
    if game_key not in all_results:
        raise ValueError(f"{game_key} not found in all_results keys: {list(all_results.keys())}")

    results_game = all_results[game_key]
    df = results_game['original_df'].copy()

    if 'author_steamid' not in df.columns:
        raise ValueError("author_steamid is missing in original_df for " + game_key)

    # Row-level cluster labels (same length as df)
    eng_res = results_game.get('engagement', {})
    play_res = results_game.get('playtime', {})

    if eng_res and eng_res.get('labels') is not None:
        df['engagement_cluster'] = eng_res['labels']
    else:
        df['engagement_cluster'] = -1

    if play_res and play_res.get('labels') is not None:
        df['playtime_cluster'] = play_res['labels']
    else:
        df['playtime_cluster'] = -1

    # Player-level clusters: reviewer & composite
    rev_res = results_game.get('reviewer', {})
    comp_res = results_game.get('composite', {})

    # Reviewer sentiment/behavior clusters
    if rev_res and 'player_stats' in rev_res and rev_res.get('labels') is not None:
        player_stats = rev_res['player_stats'].copy()
        player_stats['reviewer_cluster'] = rev_res['labels']
        df = df.merge(
            player_stats[['author_steamid', 'reviewer_cluster']],
            on='author_steamid',
            how='left'
        )
    else:
        df['reviewer_cluster'] = -1

    # Composite clusters
    if comp_res and 'player_composite' in comp_res and comp_res.get('labels') is not None:
        player_comp = comp_res['player_composite'].copy()
        player_comp['composite_cluster'] = comp_res['labels']
        df = df.merge(
            player_comp[['author_steamid', 'composite_cluster']],
            on='author_steamid',
            how='left'
        )
    else:
        df['composite_cluster'] = -1

    # Target label y
    if 'voted_up' in df.columns:
        df['y'] = df['voted_up'].astype(int)
    else:
        if 'en_bert_label_from_partial' in df.columns:
            df['y'] = (df['en_bert_label_from_partial'] == 'positive').astype(int)
        else:
            raise ValueError(f"No voted_up or sentiment label found for {game_key}")

    # Choose numeric feature columns (from engineered features)
    numeric_cols = [
        # engagement / meta
        'author_num_games_owned',
        'author_num_reviews',
        'community_participation',

        # playtime
        'author_playtime_forever',
        'author_playtime_last_two_weeks',
        'author_playtime_at_review',
        'recent_activity_ratio',
        'recent_playtime_share',

        # sentiment & helpfulness
        'avg_sentiment',
        'weighted_vote_score',
        'helpfulness_ratio',

        # purchase
        'purchase_ratio'
    ]
    numeric_cols = [c for c in numeric_cols if c in df.columns]

    # cluster columns
    cluster_cols = [
        'engagement_cluster',
        'playtime_cluster',
        'reviewer_cluster',
        'composite_cluster'
    ]

    # meta columns to keep for later grouping/analysis
    meta_cols = [c for c in ['author_steamid', 'recommendationid'] if c in df.columns]

    keep_cols = meta_cols + cluster_cols + numeric_cols + ['y']
    modeling_df = df[keep_cols].copy()

    # Ensure numeric types and fill NaN
    for c in numeric_cols:
        modeling_df[c] = pd.to_numeric(modeling_df[c], errors='coerce').fillna(0.0)

    # For cluster IDs, fill missing as -1
    for c in cluster_cols:
        modeling_df[c] = modeling_df[c].fillna(-1).astype(int)

    return modeling_df

cp_model_df  = build_modeling_table_for_game("Cyberpunk 2077", all_results)
w3_model_df  = build_modeling_table_for_game("The Witcher 3", all_results)

cp_model_df.head()
w3_model_df.head()

"""## Cyberpunk2077 model
Given a user’s gameplay and review behavior patterns (personas), what is the probability that they will submit a positive review?
"""

from sklearn.model_selection import train_test_split

# Get unique players(prevent data leakage)
unique_players = cp_model_df['author_steamid'].unique()

# Train and test groups (NO overlap)
train_players, test_players = train_test_split(
    unique_players,
    test_size=0.2,
    random_state=42
)

train_df = cp_model_df[cp_model_df['author_steamid'].isin(train_players)].copy()
test_df  = cp_model_df[cp_model_df['author_steamid'].isin(test_players)].copy()

print(f"Train rows: {len(train_df)}, Test rows: {len(test_df)}")
print(f"Unique train players: {train_df['author_steamid'].nunique()}, "
      f"Unique test players: {test_df['author_steamid'].nunique()}")

# Drop Columns
drop_cols = ['author_steamid', 'recommendationid', 'y']
drop_cols = [c for c in drop_cols if c in cp_model_df.columns]

feature_cols = [c for c in cp_model_df.columns if c not in drop_cols]

print("Feature columns used:")
print(feature_cols)

X_train = train_df[feature_cols]
y_train = train_df['y'].astype(int)

X_test = test_df[feature_cols]
y_test = test_df['y'].astype(int)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

log_clf = LogisticRegression(
    max_iter=1000,
    n_jobs=-1,
    class_weight=None
)

log_clf.fit(X_train, y_train)

y_pred = log_clf.predict(X_test)
y_proba = log_clf.predict_proba(X_test)[:, 1]

print("=== Logistic Regression Evaluation (Cyberpunk, player-level split) ===")
print(classification_report(y_test, y_pred, digits=3))
print("ROC-AUC:", roc_auc_score(y_test, y_proba))

"""## Apply to Witcher 3
If a Witcher 3 player had played Cyberpunk, how likely is their behavioral persona to leave a positive review?
"""

w3_model_df = build_modeling_table_for_game("The Witcher 3", all_results)

missing_in_w3 = [c for c in feature_cols if c not in w3_model_df.columns]
if missing_in_w3:
    print("Warning: these feature columns are missing in Witcher 3 and will be dropped:", missing_in_w3)

w3_feature_cols = [c for c in feature_cols if c in w3_model_df.columns]

X_w3 = w3_model_df[w3_feature_cols]

w3_model_df['p_pos_cyberpunk'] = log_clf.predict_proba(X_w3)[:, 1]

# Persona-level summaries for Witcher 3

w3_eng_summary = (
    w3_model_df.groupby('engagement_cluster')['p_pos_cyberpunk']
               .agg(['count', 'mean'])
               .sort_values('mean', ascending=False)
)

w3_playtime_summary = (
    w3_model_df.groupby('playtime_cluster')['p_pos_cyberpunk']
               .agg(['count', 'mean'])
               .sort_values('mean', ascending=False)
)

w3_reviewer_summary = (
    w3_model_df.groupby('reviewer_cluster')['p_pos_cyberpunk']
               .agg(['count', 'mean'])
               .sort_values('mean', ascending=False)
)

w3_persona_summary = (
    w3_model_df.groupby('composite_cluster')['p_pos_cyberpunk']
               .agg(['count', 'mean'])
               .sort_values('mean', ascending=False)
)

from IPython.display import display

print("=== Engagement Personas (Witcher 3) ===")
display(w3_eng_summary)
print("\n=== Playtime Personas (Witcher 3) ===")
display(w3_playtime_summary)
print("\n=== Reviewer Personas (Witcher 3) ===")
display(w3_reviewer_summary)
print("\n=== Composite Personas (Witcher 3) ===")
display(w3_persona_summary)

"""Witcher players and Cyberpunk players share the same behavioral personas."""

# Charts
import matplotlib.pyplot as plt

w3_persona_summary['mean'].plot(kind='bar', figsize=(8,5))
plt.title("Predicted Cyberpunk Liking Probability by Witcher Composite Persona")
plt.ylabel("Mean Probability of Positive Review")
plt.show()

"""# Defining Cyberpunk 2077 clusters"""

games_clean['cyberpunk'].groupby(eng['labels'])[
    ['author_num_games_owned', 'author_num_reviews', 'community_participation', 'author_playtime_forever']
].mean()

"""Cluster 0(W3-1): High-Ownership Veterans:

Long-time Steam users, highly engaged with the platform, familiar with many games.

Cluster 1(W3-0): Focused but Low-Ownership Gamers--Deep Single-Game Players:

These users play very few games, but play them deeply and review them fairly often.

Cluster 2(W3-2): Ultra-High Playtime Players:

High lifetime playtime; most committed, plays more than anyone else.

Cluster 3(W3-3): Active Community Reviewers:

Extremely active in community interactions and voting
"""

df_temp = games_clean['cyberpunk'].copy()

play = df_temp['author_playtime_forever'].fillna(0).astype(float)
mu, sigma = play.mean(), play.std(ddof=0)
df_temp['playtime_z'] = (play - mu) / (sigma if sigma > 0 else 1.0)
df_temp['playtime_pct'] = play.rank(method='average', pct=True)

df_temp.groupby(all_results['Cyberpunk 2077']['playtime']['labels'])[    ['author_playtime_forever','author_playtime_last_two_weeks',
     'recent_activity_ratio','playtime_z','playtime_pct','recent_playtime_share']
].mean()

"""Cluster 0(W3-0): Committed Long-Term Players

High total playtime, steady ongoing engagement

Cluster 1(W3-2): Extreme Veterans

Extremely high lifetime and recent play; most dedicated

Cluster 2(W3-1): Moderate Long-Form Players

Lower total hours; casual but consistent

Cluster 3(W3-3): Recent Binge Players

Massive spike in recent hours; update-driven activity
"""

rev = all_results['Cyberpunk 2077']['reviewer']
rev['player_stats'].groupby(rev['labels'])[
    ['avg_sentiment', 'total_reviews', 'avg_vote_score']
].mean()

"""Cluster 0(W3-0): Critical Reviewers

Cluster 1(W3-2): Extremely Positive Reviewers(Enthusiastic)

Cluster 2(W3-1): Highly Experienced Reviewers

{layers who review many games and write more neutral
"""

comp = all_results['Cyberpunk 2077']['composite']
comp['player_composite'].drop(columns=['author_steamid']).groupby(comp['labels']).mean()

"""Cluster 0(W3-0): Veteran Positive Buyers

Large game libraries, consistent purchasers, positive sentiment

Cluster 1(W3-3): Moderate Positive Players

Mid level libraries, moderate engagement, positive attitude

Cluster 2(W3-4): Active Community Activists

Critical but engaged # W3 was positive

Cluster 3(W3-2): Low-Profile Enthusiasts(Quiet players)

Low-ownership but overall positive(maybe new steam acounts)

Cluster 4(W3-1): High-Playtime Balanced Enthusiasts

High playtime, helpful reviewers, balanced sentiment

# Witcher 3 clusters
"""

df_witcher_features.groupby(
    all_results['The Witcher 3']['engagement']['labels']
)[['author_num_games_owned',
  'author_num_reviews',
  'community_participation',
  'author_playtime_forever']].mean()

df_temp = df_witcher_features.copy()

play = df_temp['author_playtime_forever'].fillna(0).astype(float)
mu, sigma = play.mean(), play.std(ddof=0)
df_temp['playtime_z'] = (play - mu) / (sigma if sigma > 0 else 1.0)
df_temp['playtime_pct'] = play.rank(method='average', pct=True)

df_temp.groupby(all_results['The Witcher 3']['playtime']['labels'])[
    ['author_playtime_forever','author_playtime_last_two_weeks',
     'recent_activity_ratio','playtime_z','playtime_pct',
     'recent_playtime_share']
].mean()

rev = all_results['The Witcher 3']['reviewer']
rev['player_stats'].groupby(rev['labels'])[
    ['avg_sentiment', 'total_reviews', 'avg_vote_score']
].mean()

comp = all_results['The Witcher 3']['composite']
comp['player_composite'].drop(columns=['author_steamid']).groupby(comp['labels']).mean()

"""# Results
Witcher 3 reviewers overwhelmingly has positive sentiment personas, and these same personas map cleanly to Cyberpunk personas.This confirms a strong shared CDPR gamer society, giving a reliably persona-level sentiment predict for future releases such as The Witcher 4 and identify at-risk segments before launch.
"""

from sklearn.model_selection import train_test_split

# 1. Get unique players
unique_players = cyber_model_df['author_steamid'].unique()

# 2. Split players into train and test groups (NO overlap)
train_players, test_players = train_test_split(
    unique_players,
    test_size=0.2,
    random_state=42
)

# 3. Build train/test sets by player ID
train_df = cyber_model_df[cyber_model_df['author_steamid'].isin(train_players)].copy()
test_df  = cyber_model_df[cyber_model_df['author_steamid'].isin(test_players)].copy()

print(f"Train rows: {len(train_df)}, Test rows: {len(test_df)}")
print(f"Unique train players: {train_df['author_steamid'].nunique()}, "
      f"Unique test players: {test_df['author_steamid'].nunique()}")